<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    
        <title>Building A News Crawler</title>
    

    <meta name="description" content="Machine Learning, Lyricism, Music, Food and everything else.">

    

    <link rel="icon" href="/assets/img/favicon.png">
    <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="/assets/css/main.css">
</head>

<body>

    <div class="wrapper">
        <div class="post">
    <a class="post__back" href="/">&lt;-- home</a>
    <h1 class="post__title">Building A News Crawler</h1>
    <p class="post__date">May 25, 2016</p>
    <div class="post__content"?>
        <p>One of the things that I enjoy doing is text mining. Basically going through huge
text corpora like a mad man searching for different patterns. In order to feed this odd habit,
I need collections of texts. There are countless public corpora available, but
building your own from scratch rewards some kind of an unexplainable joy. It might be one
of those tasks which are not too hard to be overwhelming, yet challenging enough
to be rewarding. I might be broken. I might need help. But that’s a different story.</p>

<p>I used to do this with Ruby utilizing <a href="http://www.nokogiri.org/">Nokogiri</a>. But I found out that Python has so much more tools for automating almost everything, so in this post I’m going to describe the steps that I took to build this crawler. I realize I might be using the term crawler too liberally here, but my dumb feelings are telling me the word scraper is less accurate. Scream in the comment section if you please. And some of the tools are completely new to me, so forgive me if I mess up some of the steps.</p>

<p>So, let’s get it going. First, we need to identify our source for text. Lately, I enjoy
mining news articles. And on top of that, you can almost be certain that the patterns
that you uncover are new patterns that no one else has discovered. Cool, we have a place to start.</p>

<p>The best place to get news programmatically might be RSS Feeds. RSS Feeds might not be
as hip as they once were, but most news portals still provide them. How to read these feeds?</p>

<p>I found <a href="https://pythonhosted.org/feedparser/introduction.html#parsing-a-feed-from-a-remote-url">feedparser</a>. This is pretty sweet. So install it on Python.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pip install feedparser
</code></pre>
</div>

<p>Their documentation is awesome. I got my feedparser up and running in no time. To test out my parser I will be using <a href="http://www.theborneopost.com/feed/">BorneoPost</a> as a news feed. I played around in IPython to get some familiarity.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>import feedparser
parsed = feedparser.parse("http://www.theborneopost.com/feed/")
parsed.entries
</code></pre>
</div>

<p>Nice. Now I can create a new list of links.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>links = [el["link"] for el in parsed.entries]
</code></pre>
</div>

<p>Now I need to extract the content from each link. Content extraction refers to extracting the main content from the sites. For us humans, this task is trivial. But for computers, the whole site is a HTML document. We could write a program that parses the HTML and extracts the DOM by telling the program where the main text is but this would also mean we would have to write a different program when we want to extract contents on a different site, which may have different formatting. Hence, we need the help of content extractors. They usually utilize some clever machine learning models to select which DOM most probably holds the main content. If you are interested you can <a href="http://www2013.org/companion/p89.pdf">read here</a> to get some idea.</p>

<p>I am aware of <a href="https://pypi.python.org/pypi/dragnet">dragnet</a>, so I tried to install it with pip, but received some error. Then I realized, it does not support Python 3. Then I read one of the <a href="https://moz.com/devblog/benchmarking-python-content-extraction-algorithms-dragnet-readability-goose-and-eatiht/">blog posts</a> from the developers and found out about other popular content extractors.
So I thought to myself, I might as well just try one of the others.</p>

<p>At random, I chose <a href="https://pypi.python.org/pypi/readability-lxml">python-readibility</a>. Installed it with:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pip install readability-lxml
</code></pre>
</div>

<p>Installation worked like a charm. So, I shall be sticking with this one if the results from the content extraction is reasonable and acceptable. Let’s try it out.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>from readability.readability import Document
import requests
r  = requests.get(links[0])
html = r.text
readable_article = Document(html).summary()
</code></pre>
</div>

<p>Ok, readable_article, filtered out most of the unwanted elements. But there are still some HTML tags. I don’t want that. I tried feeding <code class="highlighter-rouge">readable_article</code> back to <code class="highlighter-rouge">Document().summary()</code>, but I get the same result.</p>

<p>I remembered using <a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> with its <code class="highlighter-rouge">.text</code> method to get rid of HTML tags. I think it was from some text mining tutorial I can’t quite recall. Anyways, let’s try that out and see if it works.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>from bs4 import BeautifulSoup
cleaned_readable_article = BeautifulSoup(readable_article).text
# 'KUALA LUMPUR: 1Malaysia Development Bhd (1MDB...'
</code></pre>
</div>

<p>I’ve truncated the output a bit, but in my eyes it works. Now, we have all the tools to start building our corpus. A recap to what we will be doing.</p>

<ul>
  <li>We need a collection of RSS Feeds</li>
  <li>From the feeds, we get the links and published date of each article</li>
  <li>From the links, we extract the main text</li>
  <li>We shall then save each text in a JSON file</li>
</ul>

<p>Now all we need is to bring everything together. My example can be accessed <a href="https://gist.github.com/amirothman/e29875f0f34148ecf340d5039664ccd2">here</a>. If you utilize a while loop with a sleep, you can have yourself a nice simple crawler. Collect more news feeds and let your crawler run for some time. Watch it grow like a little baby. Be aware however, certain sites do not like the fact that you are scraping. Be polite to not DDOS their servers. We all gotta eat.</p>

<p>Hopefully next time I will be able to demonstrate some ways we can augment and explore our text data to uncover some cool patterns.</p>

    </div>
</div>
<div id="disqus_thread"></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/

var disqus_config = function () {
this.page.url = "http://amirothman.github.io"+"/2016/05/25/news-crawler.html"; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/2016/05/25/news-crawler"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//amirothmanblog.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

    </div>
    <script id="dsq-count-scr" src="//amirothmanblog.disqus.com/count.js" async></script>
</body>

</html>
