<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    
        <title>Genre Recognition With Deep Learning</title>
    

    <meta name="description" content="Machine Learning, Lyricism, Music, Food and everything else.">

    

    <link rel="icon" href="/assets/img/favicon.png">
    <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="/assets/css/main.css">
</head>

<body>

    <div class="wrapper">
        <div class="post">
    <a class="post__back" href="/">&lt;-- home</a>
    <h1 class="post__title">Genre Recognition With Deep Learning</h1>
    <p class="post__date">May 27, 2016</p>
    <div class="post__content"?>
        <h1 id="genre-recognition-with-deep-learning">Genre Recognition With Deep Learning</h1>

<p>I’ve attempted genre recognition using deep learning. In this guide, I try to describe the steps needed to be taken to run my horrible <a href="https://github.com/amirothman/genre-erkennung-pipeline">code</a>. Haha. This is mainly for the learning experience.</p>

<p>Since this is a machine learning task, we shall follow the typical procedure of:</p>

<ul>
  <li>Collecting data</li>
  <li>Feature extraction</li>
  <li>Training and testing model</li>
  <li>Querying model</li>
</ul>

<h1 id="preliminaries">Preliminaries</h1>

<p>Firstly, I am not a super l33t 10x software engineer and I cobbled up most of this code in a day.
So, a lot of restructuring and refactoring should and could be done. This guide will show how to use the code as is. Clone the git repo, and switch the branch to working-model-0.1</p>

<div class="highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/amirothman/genre-erkennung-pipeline
cd genre-erkennung-pipeline
git checkout working-model-0.1
</code></pre>
</div>

<p>Now you should have all the code. But not all the data. But first, let’s go through the requirement:</p>

<ul>
  <li>keras (deep learning framework)</li>
  <li>sklearn (machine learning library)</li>
  <li>matplotlib (plotting)</li>
  <li>youtube-dl (to test data)</li>
</ul>

<p>Outside of the python libraries we also require some command line tools:</p>

<ul>
  <li>ffmpeg (manipulating audio files)</li>
  <li>sonic-annotator (Vamp feature extraction utility)</li>
</ul>

<p>And we also need to install some Vamp plugins:</p>

<ul>
  <li><a href="https://code.soundsoftware.ac.uk/projects/qm-vamp-plugins/files">QM-Vamp Plugins</a></li>
  <li><a href="https://github.com/bbcrd/bbc-vamp-plugins/releases">BBC Vamp Plugins</a></li>
</ul>

<p>Refer to <a href="http://mtg.upf.edu/technologies/melodia?p=Download%20and%20installation">here</a> for instructions to install vamp plugins on different platforms.</p>

<h1 id="collecting-data">Collecting data</h1>

<p>Most of the methods that are written within this code would have the folder “dataset” as the default directory path for the data. Create that folder inside the main project directory.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mkdir dataset
</code></pre>
</div>

<p>Inside this directory, you should create another directory containing your dataset.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd dataset
mkdir my_data_set
</code></pre>
</div>

<p>Throughout the code base, we use the convention of train set and test set to separate our training data and testing data. Create those directories:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd my_data_set
mkdir test
mkdir train
</code></pre>
</div>

<p>Now, you can put your data in the respective directories. The files need to be separated in different folders for different genres. The code would use the name of the directory as the genre. Here is an example of how a dataset directory should look like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>└── my_data_set
    ├── test
    │   ├── genre1
    │   │   ├── 010._Ольга_Василюк_-_Нет_Тебя.mp3
    │   │   ├── 017._Eros_Ramazzotti_-_Perfetto.mp3
    │   │   └── 018._T.I._Feat._Pharrell_-_Oh_Yeah.mp3
    │   └── genre2
    │       ├── 011._Maan_-_Perfect_World__Prod._by_Hardwell_.mp3
    │       ├── 015._Глюкоза_-_Согрей.mp3
    │       └── 016._Вика_Воронина_Feat._Storm_Djs_-_Угги.mp3
    └── train
        ├── genre1
        │   ├── 004._Prides_-_Out_Of_The_Blue.mp3
        │   ├── 008._Марина_Алиева_-_Подари_Любовь.mp3
        │   └── 009._Чи-Ли_-_Ангел_На_Моём_Плече.mp3
        └── genre2
            ├── 001._Paul_Van_Dyk_-_Lights.mp3
            ├── 002._Zedd,_Bahari_-_Addicted_To_A_Memory.mp3
            └── 003._MOYYO_-_Имя_Moyyo.mp3

</code></pre>
</div>

<p>I’ve written a bash script <code class="highlighter-rouge">sample_song.sh</code> to split the dataset into test set and training set. If you are a shell ninja, you can edit the script split the dataset.</p>

<p>By the way, you can have as many genres as you want. Just make sure:</p>

<ul>
  <li>Both training set and testing have the same genres</li>
  <li>The split between test set and training set is sensible</li>
</ul>

<p>The format of the audio can be any audio format that is supported by sonic-annotator and ffmpeg.
One preprocessing step before we continue to feature extraction:</p>

<ul>
  <li>is splitting the audio files into 30 second chunks - to make it more manageable</li>
  <li>merge stereo to mono - because we theorized that the genre of a song is independent from this property</li>
</ul>

<p>To do this, we can use the <code class="highlighter-rouge">split_30_seconds_mono.py</code> script. The method to be used here is <code class="highlighter-rouge">batch_thirty_seconds(folder_path,file_format)</code>. In our case, folder_path would be <code class="highlighter-rouge">my_data_set</code> and file_format would be <code class="highlighter-rouge">mp3</code>. You should edit this in the main part of the code.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>batch_thirty_seconds(folder_path,file_format)
  folder_path: string of directory path (where the folder train and test is)
  file_format: string of audio format (e.g. 'mp3', 'au', etc.)
</code></pre>
</div>

<p>Run this script:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python split_30_seconds_mono.py
</code></pre>
</div>

<p>If your audio files contain spaces or some other weird characters, this script will throw an error. You can use the script space_to_underscore.sh to rename them.</p>

<h1 id="feature-extraction">Feature Extraction</h1>

<p>Congratulations that you got this far. Now we will do feature extraction. What we will do is first convert the audio files into CSV files with feature, then convert the CSV files into Numpy arrays and serialize them with pickle. We can actually create the numpy arrays directly, skipping the csv files but:</p>

<ul>
  <li>I was not aware of it during that time (Haha)</li>
  <li>It’s actually much slower. Probably because the vamp host is a python implementation.</li>
</ul>

<p>The python script <code class="highlighter-rouge">extract_features.py</code> will be used here. The method is extract_features;</p>

<p>extract_features(path)
    path: string of data path to do feature extraction</p>

<p>You should edit this in the main part of the code and run the script.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python extract_features.py
</code></pre>
</div>

<p>After running that script, you will realize a bunch of csv files in your dataset. It may take a while for this process to finish. Have a cup of coffee or a line of cocaine. Depends on which level of a rockstar you are.</p>

<p>Before proceeding, you are going to have to create another directory for your pickled_vectors.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mkdir pickled_vectors
</code></pre>
</div>

<p>After that step you would have a bunch of csv files. Now to convert them into numpy arrays and pickle them, so you can reuse and abuse them. For this we will turn to <code class="highlighter-rouge">parse_songs.py</code>. The method which we will utilize is <code class="highlighter-rouge">build_vectors</code></p>

<div class="highlighter-rouge"><pre class="highlight"><code>build_vectors(folder_path,keyword,lower_limit)
  folder_path: string for path of dataset
  keyword: string for keyword of feature e.g. "spectral-contrast_peaks". This will be used to match the csv file output by sonic-annotator
  lower_limit: integer for the index of column of the csv file to use. The first column is a timestamp. Sometimes, we do not want this in our array.
</code></pre>
</div>

<p>For our example:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>build_vectors(folder_path="dataset/my_data_set",keyword="spectral-contrast_peaks",lower_limit=1)
build_vectors(folder_path="dataset/my_data_set",keyword="mfcc_coefficients",lower_limit=1)
</code></pre>
</div>

<p>As before, you should edit the main part of the code. Run the code.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python parse_songs.py
</code></pre>
</div>

<p>If you check the folder pickled_vectors you should have your pickled vectors saved there.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ls pickled_vectors
</code></pre>
</div>

<p>If it is empty, you probably mess up something. Call the ambulance.</p>

<h1 id="training-model">Training Model</h1>

<p>Awesome. You’ve reached here. This is the machine learning step. The most interesting step. There are two separate models for the two different features. The models are in two different scripts, mfcc_model.py and spectral-contrast_peaks_model.py. The merged model is in the script prototype_merged.py. You can train each model separately or combining them. Let’s try just one model first, then the other one then the merged one.</p>

<h2 id="mfcc-model">MFCC Model</h2>

<p><img src="/assets/img/mfcc_model.png" alt="mfcc model" /></p>

<p>Change the X, y, X_test, and y_test variable to load the pickled vectors of your desire. If you are following this tutorial, it would be:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>X = pickle.load(open("pickled_vectors/mfcc_coefficients_training_vector.pickle","rb"))
y = pickle.load(open("pickled_vectors/mfcc_coefficients_label.pickle","rb"))

X_test = pickle.load(open("pickled_vectors/mfcc_coefficients_evaluation_training_vector.pickle","rb"))
y_test = pickle.load(open("pickled_vectors/mfcc_coefficients_evaluation_label.pickle","rb"))
</code></pre>
</div>

<p>Now you can run this model with:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python mfcc_model.py
</code></pre>
</div>

<p>Or if you have configured CUDA on your machine, you can also use keras_gpu.sh. This is probably the wrongest hackiest way to run Theano code with Cuda but, it works for now.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sh keras_gpu.sh mfcc_model.py
</code></pre>
</div>

<p>One thing that you may have to further edit is the number of outputs of your neural network. On this line:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>model.add(Dense(2))
</code></pre>
</div>

<p>2 refers to 2 genres. If your dataset have 10 genres, change it 10.</p>

<h2 id="spectral-contrast-model">Spectral Contrast Model</h2>

<p><img src="/assets/img/spectral-contrast_peaks_model.png" alt="spectral contrast model" /></p>

<p>As before, change the X, y, X_test, and y_test variable to load the pickled vectors of your desire. For our example:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>X = pickle.load(open("pickled_vectors/spectral-contrast_peaks_training_vector.pickle","rb"))
y = pickle.load(open("pickled_vectors/spectral-contrast_peaks_label.pickle","rb"))

X_test = pickle.load(open("pickled_vectors/spectral-contrast_peaks_evaluation_training_vector.pickle","rb"))
y_test = pickle.load(open("pickled_vectors/spectral-contrast_peaks_evaluation_label.pickle","rb"))
</code></pre>
</div>

<p>Now you can run this model with:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python spectral_contrast_peaks_model.py
</code></pre>
</div>

<p>Or if you have configured CUDA on your machine, you can also use keras_gpu.sh. This is probably the wrongest hackiest way to run Theano code with Cuda but, it works for now.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sh keras_gpu.sh spectral_contrast_peaks_model.py
</code></pre>
</div>

<p>One thing that you may have to further edit is the number of outputs of your neural network. On this line:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>model.add(Dense(2))
</code></pre>
</div>

<p>2 refers to 2 genres. If your dataset have 10 genres, change it 10.</p>

<h2 id="merged-model">Merged Model</h2>

<p><img src="/assets/img/model.png" alt="merged model" /></p>

<p>Here, we have merged model. It combining the two previous model and concatenate them into one model. We have two different example (X vectors), so a bit different than before, we will have to change X_1, X_2, X_test_1, X_test_2, y and y_test. For our example:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>y = pickle.load(open("pickled_vectors/mfcc_coefficients_label.pickle","rb"))
y_test = pickle.load(open("pickled_vectors/mfcc_coefficients_evaluation_label.pickle","rb"))

X_1 = pickle.load(open("pickled_vectors/mfcc_coefficients_training_vector.pickle","rb"))
X_test_1 = pickle.load(open("pickled_vectors/mfcc_coefficients_evaluation_training_vector.pickle","rb"))

X_2 = pickle.load(open("pickled_vectors/spectral-contrast_peaks_training_vector.pickle","rb"))
X_test_2 = pickle.load(open("pickled_vectors/spectral-contrast_peaks_evaluation_training_vector.pickle","rb"))
</code></pre>
</div>

<p>One thing that you may have to further edit is the number of outputs of your neural network. On this line:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>model.add(Dense(2))
</code></pre>
</div>

<p>2 refers to 2 genres. If your dataset have 10 genres, change it 10.</p>

<p>Before running this model, you may want to save the weights some where. Let’s create that directory.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mkdir model_weights
</code></pre>
</div>

<p>Uncomment the following line:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>final_model.save_weights("model_weights/merged_model_weights.hdf5",overwrite=True)
</code></pre>
</div>

<p>Now you can run this model with:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python prototype_merged.py
</code></pre>
</div>

<p>Or if you have configured CUDA on your machine, you can also use keras_gpu.sh. This is probably the wrongest hackiest way to run Theano code with Cuda but, it works for now.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sh keras_gpu.sh prototype_merged.py
</code></pre>
</div>

<p>If everything goes well, I would like to congratulate you again. You have successfully trained a deep learning model. It’s an ok-ish model, but at this point you can already use fancy buzz words like deep learning, artificial intelligence or the same technology behind google’s deepmind.</p>

<p>Awesome.</p>

<h1 id="querying-the-model">Querying The Model</h1>

<p>Now you would like to ask the model. By giving it a song, what would the model predict? Firstly, the song which the model would need to predict, would have to go through the whole pipeline.</p>

<p>The merge model, saved it’s architecture into a json file. This happened in the following part of the prototype_merged.py</p>

<div class="highlighter-rouge"><pre class="highlight"><code>  json_string = final_model.to_json()
  with open("model_architecture/merged_model_architecture.json","w") as f:
      f.write(json.dumps(json_string, sort_keys=True,indent=4, separators=(',', ': ')))
</code></pre>
</div>

<p>The trained weights should be saved in the following hdf5 file:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>model_weights/merged_model_weights.hdf5
</code></pre>
</div>

<p>So what we have to do to query the model are:</p>

<ul>
  <li>extract the features from the song</li>
  <li>load the model architecture</li>
  <li>load the model weights</li>
  <li>get some prediction</li>
</ul>

<p>For this purpose, the script querying_genre.py is made. Make sure the right architecture is loaded:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>json_string = json.load(open("model_architecture/merged_model_architecture.json","r"))
</code></pre>
</div>

<p>And also the right weights:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>model.load_weights("model_weights/merged_model_weights.hdf5")
</code></pre>
</div>

<p>Now for the song in question. Let’s put that song inside dataset in the folder query</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd dataset
mkdir query
</code></pre>
</div>

<p>We shall create another folder for this one particular query.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd query
mkdir song_1
</code></pre>
</div>

<p>Let’s grab a song on Youtube with youtube-dl.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>youtube-dl -x https://www.youtube.com/watch?v=VDvr08sCPOc
</code></pre>
</div>

<p>For hacky reasons, the filename of the song needs to correspond to the name of the folder.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mv Remember\ The\ Name\ \(Official\ Video\)\ -\ Fort\ Minor-VDvr08sCPOc.opus song_1.opus
</code></pre>
</div>

<p>sonic-annotator does not support the opus file format. We are going to need to convert it to mp3 , or what ever format that you like as long as sonic-annotator supports it.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ffmpeg -i song_1.opus song_1.mp3
</code></pre>
</div>

<p>We are going to have delete the dot opus file. Again, for hacky my-code-kinda-sucks reasons.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rm song_1.opus
</code></pre>
</div>

<p>Now let’s edit querying_genre.py so that it corresponds to our song query.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>song_name_without_ending = "dataset/query/song_1/song_1"
file_format = "mp3"
song_folder = "dataset/query/song_1"

</code></pre>
</div>

<p>Let’s run this script.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python querying_genre.py
</code></pre>
</div>

<p>An example of the output would be something like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[1 1 0 0 1 0 0 1]
</code></pre>
</div>

<p>What does this mean? The song is split into 30 seconds chunks. The model predicts genre_1 for the first two 30 second chunks and genre_2 for the following. Of course we can do a lot more cleverer and more sophisticated ways to determine what the model determine. But if we take the mode of the output as the model’s decision, we can say that the song that we had, was of genre_2.</p>

<p>Hope you had fun following this tutorial. It is useful to read the documentation of Keras if you want tweak the model.</p>

    </div>
</div>
<div id="disqus_thread"></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/

var disqus_config = function () {
this.page.url = "http://amirothman.github.io"+"/2016/05/27/genre-recognition-with-deep-learning.html"; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/2016/05/27/genre-recognition-with-deep-learning"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//amirothmanblog.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

    </div>
    <script id="dsq-count-scr" src="//amirothmanblog.disqus.com/count.js" async></script>
</body>

</html>
